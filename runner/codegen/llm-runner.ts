import {z} from 'zod';
import {
  LlmGenerateFilesRequest,
  LlmResponseFile,
  ToolLogEntry,
  Usage,
} from '../shared-interfaces.js';
import {UserFacingError} from '../utils/errors.js';
import {Executor} from '../orchestration/executors/executor.js';

export async function assertValidModelName(value: string, executor: Executor) {
  const checkResult = await executor.isSupportedModel(value);
  if (!checkResult.supported) {
    throw new UserFacingError(
      `Unsupported model specified.${
        checkResult.availableModels !== undefined
          ? ` Available models:\n` + checkResult.availableModels.map(m => `- ${m}`).join('\n')
          : ''
      }`,
    );
  }
}

/**
 * Interface for running a large language model.
 */
export interface LlmRunner {
  /** Unique ID of the runner. */
  readonly id: string;

  /** Display name of the runner. */
  readonly displayName: string;

  /**
   * Whether the runner is able to run a repair loop on its own or if it
   * should be triggered by the eval infrastructure via prompting.
   */
  readonly hasBuiltInRepairLoop: boolean;

  /** Sends a file generation request to the LLM. */
  generateFiles(
    options: LocalLlmGenerateFilesRequestOptions,
  ): Promise<LocalLlmGenerateFilesResponse>;

  /** Sends a normal text generation request to the LLM. */
  generateText(options: LocalLlmGenerateTextRequestOptions): Promise<LocalLlmGenerateTextResponse>;

  /** Sends a schema-constrained generation request to the LLM. */
  generateConstrained<T extends z.ZodTypeAny = z.ZodTypeAny>(
    options: LocalLlmConstrainedOutputGenerateRequestOptions<T>,
  ): Promise<LocalLlmConstrainedOutputGenerateResponse<T>>;

  /** Gets the names of the models supported by the runner. */
  getSupportedModels(): string[];

  /**
   * Starts all of the specified MCP servers.
   * Optional since not all runners may support MCP.
   * @param hostName Name for the MCP host.
   * @param servers Configured servers that should be started.
   * @returns Details about the created server.
   */
  startMcpServerHost?(hostName: string, servers: McpServerOptions[]): Promise<McpServerDetails>;

  /** Stops tracking MCP server logs and returns the current ones. */
  flushMcpServerLogs?(): string[];

  /** Cleans up any resources taken up by the runner. */
  dispose(): Promise<void>;
}

interface BaseLlmRequestOptions {
  /** Name of the model to use. */
  model: string;
  /** Whether to skip sending the requests to the running MCPs. */
  skipMcp?: boolean;
  /** Optional messages to be passed along as context. */
  messages?: PromptDataMessage[];
  /** Configures a timeout for the request. */
  timeout?: {
    /** Message to be logged if the request times out. */
    description: string;
    /** Duration of the timeout in minutes. */
    durationInMins: number;
  };
  /** Configuration for the "thinking mode" of a model. */
  thinkingConfig?: {
    /** Whether to capture thoughts and return them.  */
    includeThoughts?: boolean;
  };
  /** Signal to fire when this LLM request should be aborted. */
  abortSignal: AbortSignal;
}

/** Options needed to send a text generation request. */
export interface LocalLlmGenerateTextRequestOptions extends BaseLlmRequestOptions {
  /** Prompt to send. */
  prompt: string;
}

/** Context needed for an file generation context. */
export interface LocalLlmGenerateFilesContext extends LlmGenerateFilesRequest {
  /** Command that the LLM can use to verify that the build works. */
  buildCommand: string | undefined;
  /** Package manager that the LLM can use. */
  packageManager: string | undefined;
  /** All available package managers supported by the runner. */
  possiblePackageManagers: string[];
}

/** Options needed to send a file generation request. */
export interface LocalLlmGenerateFilesRequestOptions extends BaseLlmRequestOptions {
  /** Context necessary for the request. */
  context: LocalLlmGenerateFilesContext;
}

/**
 * Options that can be passed for a schema-constrained generation
 * request to an LLM.
 */
export interface LocalLlmConstrainedOutputGenerateRequestOptions<
  T extends z.ZodTypeAny = z.ZodTypeAny,
> extends BaseLlmRequestOptions {
  /** Prompt to send. */
  prompt: string;
  /** Schema that the response should conform to. */
  schema: T;
}

/** Constrained output response by the LLM. */
export interface LocalLlmConstrainedOutputGenerateResponse<T extends z.ZodTypeAny = z.ZodTypeAny> {
  /** Result generated by the LLM. */
  output: z.infer<T> | null;
  /** Token usage data, if available. */
  usage?: Partial<Usage>;
  /** Reasoning messages from the LLM. */
  reasoning: string;
}

/** LLM response. */
interface BaseLlmGenerateResponse {
  /** Token usage data, if available. */
  usage?: Partial<Usage>;
  /** Reasoning messages from the LLM. */
  reasoning: string;
  /** Tool requests and responses. */
  toolLogs?: ToolLogEntry[];
}

/** File generation response from the LLM. */
export interface LocalLlmGenerateFilesResponse extends BaseLlmGenerateResponse {
  files: LlmResponseFile[];
}

/** Text response from the LLM. */
export interface LocalLlmGenerateTextResponse extends BaseLlmGenerateResponse {
  text: string;
}

/** Schema for the LLM server options. */
export const mcpServerOptionsSchema = z.object({
  /** Name of the server. */
  name: z.string(),
  /** Command that starts the server. */
  command: z.string(),
  /** Arguments to pass into the command. */
  args: z.array(z.string()),
  /** Environment variables to use when executing the command */
  env: z.record(z.string(), z.string()).optional(),
});

/** Options used to start an MCP server. */
export type McpServerOptions = z.infer<typeof mcpServerOptionsSchema>;

/** Details about an MCP server. */
export interface McpServerDetails {
  tools: string[];
  resources: string[];
}

/**
 * Type for a prompt message may be passed to LLM runner in the eval tool.
 *
 * A more specialized type allows us to count tokens without having to
 * deal with conversions from complex prompt data to e.g. OpenAI message data.
 * */
export interface PromptDataMessage {
  role: 'user' | 'model';
  content: Array<{text: string} | {media: {url: string; base64PngImage: string}}>;
}
