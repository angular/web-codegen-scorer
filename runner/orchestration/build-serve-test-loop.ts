import PQueue from 'p-queue';
import {LocalLlmGenerateFilesResponse} from '../codegen/llm-runner.js';
import {BuildResultStatus} from '../workers/builder/builder-types.js';
import {Environment} from '../configuration/environment.js';
import {
  AssessmentConfig,
  AttemptDetails,
  LlmContextFile,
  RootPromptDefinition,
} from '../shared-interfaces.js';
import {ProgressLogger} from '../progress/progress-logger.js';
import {BuildType, runBuild} from './build-worker.js';
import {EvalID} from './executors/executor.js';
import {serveAndTestApp} from './serve-testing-worker.js';
import {runTest} from './test-worker.js';
import {BrowserAgentTaskInput} from '../testing/browser-agent/models.js';
import {
  DEFAULT_MAX_BUILD_REPAIR_ATTEMPTS,
  DEFAULT_MAX_TEST_REPAIR_ATTEMPTS,
} from '../configuration/constants.js';
import {repairAndBuild} from './repair.js';

/**
 * Attempts to build and test the code that an LLM generated.
 *
 *  * If the build fails, attempts to fix the breakage and build again.
 *  * If tests fail (like Axe or project tests), we may repair and retry.
 *
 * @param config Assessment config.
 * @param evalID ID of the eval being attempted for build.
 * @param env Environment that is currently being run.
 * @param rootPromptDef Definition of the root prompt.
 * @param directory Directory on disk to which to write.
 * @param contextFiles Files that should be passed as context to the LLM.
 * @param initialOutputFiles Initial files generated by the LLM.
 * @param usage Usage data from the initial LLM run.
 * @param attemptDetails Array tracking information about the current build attempt.
 * @param skipScreenshots Whether to skip taking screenshots of the app.
 * @param skipAxeTesting Whether or not to skip Axe testing of the app.
 * @param abortSignal Signal to fire when the build should be aborted.
 * @param workerConcurrencyQueue Concurrency queue for controlling parallelism of worker invocations (as they are more expensive than LLM calls).
 */
export async function attemptBuildAndTest(
  config: AssessmentConfig,
  evalID: EvalID,
  env: Environment,
  rootPromptDef: RootPromptDefinition,
  directory: string,
  contextFiles: LlmContextFile[],
  initialResponse: LocalLlmGenerateFilesResponse,
  attemptDetails: AttemptDetails[],
  abortSignal: AbortSignal,
  workerConcurrencyQueue: PQueue,
  progress: ProgressLogger,
  userJourneyAgentTaskInput: BrowserAgentTaskInput | undefined,
) {
  const initialBuildResult = await runBuild(
    evalID,
    directory,
    env,
    rootPromptDef,
    abortSignal,
    workerConcurrencyQueue,
    progress,
    BuildType.INITIAL_BUILD,
  );
  let repairAttempts = 0;
  let maxRepairAttempts: number;
  let maxTestRepairAttempts: number;

  if (await env.executor.shouldRepairFailedBuilds(evalID)) {
    maxRepairAttempts = config.maxBuildRepairAttempts ?? DEFAULT_MAX_BUILD_REPAIR_ATTEMPTS;
    maxTestRepairAttempts = config.maxTestRepairAttempts ?? DEFAULT_MAX_TEST_REPAIR_ATTEMPTS;
  } else {
    maxRepairAttempts = maxTestRepairAttempts = 0;
  }

  const initialAttempt = {
    outputFiles: initialResponse.files,
    usage: {
      ...{inputTokens: 0, outputTokens: 0, totalTokens: 0, thinkingTokens: 0},
      ...initialResponse.usage,
    },
    reasoning: initialResponse.reasoning,
    buildResult: initialBuildResult,
    serveTestingResult: null,
    attempt: 0,
  };
  attemptDetails.push(initialAttempt);

  let lastAttempt: AttemptDetails = initialAttempt;
  while (
    lastAttempt.buildResult.status !== BuildResultStatus.SUCCESS &&
    repairAttempts < maxRepairAttempts
  ) {
    repairAttempts++;
    progress.log(
      rootPromptDef,
      'build',
      `Trying to repair app build (attempt #${repairAttempts + 1})`,
    );

    const attempt = await repairAndBuild(
      evalID,
      config.model,
      env,
      rootPromptDef,
      directory,
      lastAttempt.outputFiles,
      [
        {
          errorContext: 'There are the following build errors:',
          errorMessage: lastAttempt.buildResult.message,
        },
      ],
      contextFiles,
      abortSignal,
      workerConcurrencyQueue,
      repairAttempts,
      progress,
      'build',
    );

    attemptDetails.push(attempt);
    lastAttempt = attempt;
  }

  if (lastAttempt.buildResult.status === BuildResultStatus.SUCCESS) {
    // Now that we got a working app, try to serve it and collect
    // findings from the running app.
    lastAttempt.serveTestingResult = await serveAndTestApp(
      config,
      evalID,
      directory,
      env,
      rootPromptDef,
      workerConcurrencyQueue,
      abortSignal,
      progress,
      userJourneyAgentTaskInput,
    );
    lastAttempt.testResult =
      (await runTest(
        env,
        evalID,
        directory,
        rootPromptDef,
        abortSignal,
        workerConcurrencyQueue,
        progress,
      )) ?? undefined;
  }

  // Attempt to repair testing. This only runs when the last build
  // passed and serving did run. Note: By default, we don't run repair
  // attempts as it's not commonly done by LLMs in the ecosystem.
  let axeRepairAttempts = 0;
  let testRepairAttempts = 0;
  for (let testRepairAttempt = 0; testRepairAttempt < maxTestRepairAttempts; testRepairAttempt++) {
    const hasAxeFailure =
      lastAttempt.serveTestingResult && lastAttempt.serveTestingResult.axeViolations?.length;
    const hasTestFailure = lastAttempt.testResult && !lastAttempt.testResult.passed;
    if (!hasAxeFailure && !hasTestFailure) {
      break;
    }

    const attemptId = testRepairAttempt + repairAttempts + 1;

    const errors: Array<{errorContext: string; errorMessage: string}> = [];
    if (hasAxeFailure) {
      axeRepairAttempts++;
      progress.log(
        rootPromptDef,
        'build',
        `Trying to repair axe accessibility violations (attempt #${attemptId})...`,
      );
      const axeViolationsError = JSON.stringify(
        lastAttempt.serveTestingResult!.axeViolations,
        null,
        2,
      );
      progress.log(rootPromptDef, 'error', 'Found Axe accessibility violations');
      errors.push({
        errorContext:
          'There are the following accessibility errors from axe accessibility violations:',
        errorMessage: axeViolationsError,
      });
    }
    if (hasTestFailure) {
      testRepairAttempts++;
      progress.log(
        rootPromptDef,
        'project-test',
        `Trying to repair test failures (attempt #${attemptId})...`,
      );

      errors.push({
        errorContext: 'Application tests failed. Attempt to fix them. Test output was:',
        errorMessage: lastAttempt.testResult!.output,
      });
    }

    const attempt = await repairAndBuild(
      evalID,
      config.model,
      env,
      rootPromptDef,
      directory,
      lastAttempt.outputFiles,
      errors,
      contextFiles,
      abortSignal,
      workerConcurrencyQueue,
      attemptId,
      progress,
      'test',
    );

    let hasBuildFailure = attempt.buildResult.status !== BuildResultStatus.SUCCESS;
    attempt.buildFailedDuringTestRepair = hasBuildFailure;
    attemptDetails.push(attempt);
    lastAttempt = attempt;
    // If we somehow introduced build errors via the repair loop, we abort
    // further repairs and capture the failed build. This is useful insight
    // as LLMs seem to regress when asked to repair violations.
    if (hasBuildFailure) {
      break;
    }

    // Re-run serving & tests after repair.
    lastAttempt.serveTestingResult = await serveAndTestApp(
      config,
      evalID,
      directory,
      env,
      rootPromptDef,
      workerConcurrencyQueue,
      abortSignal,
      progress,
      userJourneyAgentTaskInput,
    );
    lastAttempt.testResult =
      (await runTest(
        env,
        evalID,
        directory,
        rootPromptDef,
        abortSignal,
        workerConcurrencyQueue,
        progress,
      )) ?? undefined;

    if (hasAxeFailure && lastAttempt.serveTestingResult?.axeViolations?.length === 0) {
      progress.log(rootPromptDef, 'success', `Successfully fixed all Axe accessibility violations`);
    }
    if (hasTestFailure && lastAttempt.testResult?.passed) {
      progress.log(rootPromptDef, 'success', `Successfully fixed all test failures`);
    }
  }

  return {
    buildResult: lastAttempt.buildResult,
    serveTestingResult: lastAttempt.serveTestingResult,
    outputFiles: lastAttempt.outputFiles,
    repairAttempts,
    axeRepairAttempts: axeRepairAttempts,
    testResult: lastAttempt.testResult,
    testRepairAttempts: testRepairAttempts,
  };
}
